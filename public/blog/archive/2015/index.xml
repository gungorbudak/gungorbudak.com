<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2015 on Güngör Budak</title>
    <link>https://www.gungorbudak.com/blog/archive/2015/</link>
    <description>Recent content in 2015 on Güngör Budak</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 29 Oct 2015 23:18:04 +0300</lastBuildDate><atom:link href="https://www.gungorbudak.com/blog/archive/2015/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MiClip 1.3 Installation</title>
      <link>https://www.gungorbudak.com/blog/2015/10/29/miclip-13-installation/</link>
      <pubDate>Thu, 29 Oct 2015 23:18:04 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/10/29/miclip-13-installation/</guid>
      <description>MiClip is a CLIP-seq data peak calling algorithm implemented in R but currently it doesn’t show up in the CRAN but you can obtain it from the archive and install from the source or tar.gz file.
Download the tar.gz file:
wget https://cran.r-project.org/src/contrib/Archive/MiClip/MiClip_1.3.tar.gz Start R:
R Install dependencies:
install.packages(&amp;#34;moments&amp;#34;) install.packages(&amp;#34;VGAM&amp;#34;) Finally install MiClip 1.3:
install.packages(&amp;#34;MiClip_1.3.tar.gz&amp;#34;, repos = NULL, type=&amp;#34;source&amp;#34;) Then you can test it by loading the package and viewing its help file.</description>
    </item>
    
    <item>
      <title>Generating 2D SVG Images of MOL Files using RDKit Transparent Background</title>
      <link>https://www.gungorbudak.com/blog/2015/09/16/generating-2d-svg-images-of-mol-files/</link>
      <pubDate>Wed, 16 Sep 2015 14:09:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/09/16/generating-2d-svg-images-of-mol-files/</guid>
      <description>The latest release of RDKit (2015-03) can generate SVG images with several lines of codes but by default the generated SVG image has a white background. The investigations on sources didn&amp;rsquo;t solve my problem as I couldn&amp;rsquo;t find any option for setting background to transparent background.
An example of SVG image generation can be found on RDKit blog post called New Drawing Code.
In [3] shows the SVG image generation and it returns the SVG file content in XML.</description>
    </item>
    
    <item>
      <title>Install Cairo Graphics and PyCairo on Ubuntu 14.04 / Linux Mint 17</title>
      <link>https://www.gungorbudak.com/blog/2015/09/16/install-cairo-graphics-and-pycairo-on/</link>
      <pubDate>Wed, 16 Sep 2015 08:42:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/09/16/install-cairo-graphics-and-pycairo-on/</guid>
      <description>Cairo is a 2D graphics library implemented as a library written in the C programming language but if you&amp;rsquo;d like to use Python programming language, you should also install Python bindings for Cairo.
This guide will go through installation of Cairo Graphics library version 1.14.2 (most recent) and py2cairo Python bindings version 1.10.1 (also most recent).
Install Cairo
It&amp;rsquo;s very easy with the following repository. Just add it, update your packages and install.</description>
    </item>
    
    <item>
      <title>Install RDKit 2015-03 Build on Ubuntu 14.04 / Linux Mint 17</title>
      <link>https://www.gungorbudak.com/blog/2015/09/16/install-rdkit-2015-03-build-on-ubuntu/</link>
      <pubDate>Wed, 16 Sep 2015 07:44:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/09/16/install-rdkit-2015-03-build-on-ubuntu/</guid>
      <description>RDKit is an open source toolkit for cheminformatics. It has many functionalities to work with chemical files.
Follow the below guide to install RDKit 2015-03 build on an Ubuntu 14.04 / Linux Mint 17 computer. Since Ubuntu packages don’t have the latest RDKit for trusty, you have to build RDKit from its source.
Install Dependencies
sudo apt-get install flex bison build-essential python-numpy cmake python-dev sqlite3 libsqlite3-dev libboost1.54-all-dev Download the Build</description>
    </item>
    
    <item>
      <title>Generating 2D Images of Molecules from MOL Files using Open Babel</title>
      <link>https://www.gungorbudak.com/blog/2015/08/30/generating-2d-images-of-molecules-from/</link>
      <pubDate>Sun, 30 Aug 2015 06:41:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/08/30/generating-2d-images-of-molecules-from/</guid>
      <description>Open Babel is a tool to work with molecular data in any way from converting one type to another, analyzing, molecular modeling, etc. It also has a method to convert MOL files into SVG or PNG images to represent them as 2D images.
Install Open Babel in Linux as following or go to their page for different operating systems
sudo apt-get install openbabel Open Babel uses the same command to generate SVG or PNG and recognizes the file format using the given filename to as the output option -O.</description>
    </item>
    
    <item>
      <title>Simple Way of Python&#39;s subprocess.Popen with a Timeout Option</title>
      <link>https://www.gungorbudak.com/blog/2015/08/30/simple-way-of-pythons-subprocesspopen/</link>
      <pubDate>Sun, 30 Aug 2015 06:13:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/08/30/simple-way-of-pythons-subprocesspopen/</guid>
      <description>subprocess module in Python provides us a variety of methods to start a process from a Python script. We may use these methods to run an external commands / programs, collect their output and manage them. An example use of it might be as following:
from subprocess import Popen, PIPE p = Popen([&amp;#39;ls&amp;#39;, &amp;#39;-l&amp;#39;], stdout=PIPE, stderr=PIPE) stdout, stderr = p.communicate() print stdout, stderr These lines can be used to run ls -l command in Terminal and collect the output (standard output and standard error) in stdout and stderr variables using communicate method defined in the process.</description>
    </item>
    
    <item>
      <title>Running StarCluster Load Balancer in Background in Linux</title>
      <link>https://www.gungorbudak.com/blog/2015/08/16/running-starcluster-load-balancer-in-background-in/</link>
      <pubDate>Sun, 16 Aug 2015 00:06:27 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/08/16/running-starcluster-load-balancer-in-background-in/</guid>
      <description>StarCluster loadbalancer command is regularly monitors the jobs in queue and it adds or removes nodes to the master node that is created beforehand to effectively complete the queue.
To run in in the background without killing it when the terminal closed:
nohup starcluster loadbalance cluster_name &amp;gt;loadbalance.log 2&amp;gt;&amp;amp;1 &amp;amp; or to keep standard output and standard error logs separate:
nohup starcluster loadbalance cluster_name &amp;gt; loadbalance.access.log 2&amp;gt; loadbalance.error.log &amp;amp; This will start the process and output the process ID (PID) which can be used to check or kill it.</description>
    </item>
    
    <item>
      <title>Change Apache’s Default User www-data or Home Directory /var/www/</title>
      <link>https://www.gungorbudak.com/blog/2015/08/15/change-apaches-default-user-www-data-or-home/</link>
      <pubDate>Sat, 15 Aug 2015 14:41:41 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/08/15/change-apaches-default-user-www-data-or-home/</guid>
      <description>I was getting errors from StarCluster run due to not being able to find .starcluster directory in /var/www/.
This directory has config file and log directories for StarCluster so without it, it can’t run.
To solve the issue, I set up my own user in Apache envvars instead of www-data which also changes default home directory to mine.
Edit following file with super user permissions:
sudo nano /etc/apache2/envvars Enter your username to following lines and save:</description>
    </item>
    
    <item>
      <title>Transfer Files to Your AWS S3 Storage in Linux</title>
      <link>https://www.gungorbudak.com/blog/2015/08/12/transfer-files-to-your-aws-s3-storage-in-linux/</link>
      <pubDate>Wed, 12 Aug 2015 11:00:08 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/08/12/transfer-files-to-your-aws-s3-storage-in-linux/</guid>
      <description>Uploading files to an AWS S3 storage can be difficult through the GUI with many files included or if your files are in a server where you don&amp;rsquo;t have a GUI option. Use following tool to transfer files to an S3 bucket.
Download following tool and install:
cd ~/Downloads git clone https://github.com/s3tools/s3cmd.git cd s3cmd/ sudo python setup.py install Next, execute following to create a configuration file to connect to your AWS S3 account:</description>
    </item>
    
    <item>
      <title>ImportError: Reportlab Version 2.1&#43; is needed</title>
      <link>https://www.gungorbudak.com/blog/2015/08/06/importerror-reportlab-version-21-is-needed/</link>
      <pubDate>Thu, 06 Aug 2015 14:52:10 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/08/06/importerror-reportlab-version-21-is-needed/</guid>
      <description>Little bug in xhtml2pdf version 0.0.5. To fix:
$ sudo nano /usr/local/lib/python2.7/dist-packages/xhtml2pdf/util.py Change the following lines:
if not (reportlab.Version[0] == &amp;#34;2&amp;#34; and reportlab.Version[2] &amp;gt;= &amp;#34;1&amp;#34;): raise ImportError(&amp;#34;Reportlab Version 2.1+ is needed!&amp;#34;) REPORTLAB22 = (reportlab.Version[0] == &amp;#34;2&amp;#34; and reportlab.Version[2] &amp;gt;= &amp;#34;2&amp;#34;) With these lines:
if not (reportlab.Version[:3] &amp;gt;= &amp;#34;2.1&amp;#34;): raise ImportError(&amp;#34;Reportlab Version 2.1+ is needed!&amp;#34;) REPORTLAB22 = (reportlab.Version[:3] &amp;gt;= &amp;#34;2.1&amp;#34;) </description>
    </item>
    
    <item>
      <title>Django Migrations Table Already Exists Fix</title>
      <link>https://www.gungorbudak.com/blog/2015/07/31/django-migrations-table-already-exists-fix/</link>
      <pubDate>Fri, 31 Jul 2015 14:12:11 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/07/31/django-migrations-table-already-exists-fix/</guid>
      <description>Fix this issue by faking the migrations:
python manage.py migrate –fake &amp;lt;appname&amp;gt; Taken from this SO answer </description>
    </item>
    
    <item>
      <title>Mezzanine BS Banners Translation with django-modeltranslation</title>
      <link>https://www.gungorbudak.com/blog/2015/07/01/mezzanine-bs-banners-translation-with/</link>
      <pubDate>Wed, 01 Jul 2015 14:30:29 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/07/01/mezzanine-bs-banners-translation-with/</guid>
      <description>Mezzanine BS Banners is a nice app for implementing Bootstrap 3 banners/sliders to your Mezzanine projects. The Banners model in BS Banners app has a title and its stacked inline Slides model has title and content for translation.
After [installing and setting up Django/Mezzanine translations]({% post_url 2015-07-01-djangomezzanine-content-translation-for-mezzanine %}):
Create a translation.py inside your Mezzanine project or your custom theme/skin application and copy/paste following lines:
from modeltranslation.translator import translator from mezzanine.core.translation import TranslatedSlugged, TranslatedRichText from mezzanine_bsbanners.</description>
    </item>
    
    <item>
      <title>Django/Mezzanine Content Translation for Mezzanine Built-in Applications</title>
      <link>https://www.gungorbudak.com/blog/2015/07/01/djangomezzanine-content-translation-for-mezzanine/</link>
      <pubDate>Wed, 01 Jul 2015 12:18:05 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/07/01/djangomezzanine-content-translation-for-mezzanine/</guid>
      <description>As Mezzanine comes with additional Django applications such as pages, galleries and to translate their content, Mezzanine supports django-modeltranslation integration.
Install django-modeltranslation:
pip install django-modeltranslation Add following to the INSTALLED_APPS in settings.py:
&amp;#34;modeltranslation&amp;#34;, And following in settings.py:
USE_MODELTRANSLATION = True Also, move mezzanine.pages to the top of other Mezzanine apps in INSTALLED_APPS in settings.py like so:
&amp;#34;mezzanine.pages&amp;#34;, &amp;#34;mezzanine.boot&amp;#34;, &amp;#34;mezzanine.conf&amp;#34;, &amp;#34;mezzanine.core&amp;#34;, &amp;#34;mezzanine.generic&amp;#34;, &amp;#34;mezzanine.blog&amp;#34;, &amp;#34;mezzanine.forms&amp;#34;, &amp;#34;mezzanine.galleries&amp;#34;, &amp;#34;mezzanine.twitter&amp;#34;, &amp;#34;mezzanine.accounts&amp;#34;, &amp;#34;mezzanine.mobile&amp;#34;, Run following to create fields in database tables for translations:</description>
    </item>
    
    <item>
      <title>Convert XLS/XLSX to CSV in Bash</title>
      <link>https://www.gungorbudak.com/blog/2015/06/27/convert-xlsxlsx-to-csv-in-bash/</link>
      <pubDate>Sat, 27 Jun 2015 03:16:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/06/27/convert-xlsxlsx-to-csv-in-bash/</guid>
      <description>In most of the modern Linux distributions, Libre Office is available and it can be used to convert XLS or XLSX file(s) to CSV file(s) in bash.
For XLS file(s):
for i in *.xls; do libreoffice --headless --convert-to csv &amp;#34;$i&amp;#34;; done For XLSX file(s):
for i in *.xlsx; do libreoffice --headless --convert-to csv &amp;#34;$i&amp;#34;; done You may get following warning but it still works fine:
javaldx: Could not find a Java Runtime Environment!</description>
    </item>
    
    <item>
      <title>Setting Up Templates and Python Scripts for Translation</title>
      <link>https://www.gungorbudak.com/blog/2015/06/13/setting-up-templates-and-python-scripts-for/</link>
      <pubDate>Sat, 13 Jun 2015 09:45:56 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/06/13/setting-up-templates-and-python-scripts-for/</guid>
      <description>Templates need following template tag:
{% raw %}{% load i18n %}{% endraw %} Then, wrapping any text with
{% raw %}{% trans &amp;#34;TEXT&amp;#34; %}{% endraw %} will make it translatable via Rosetta Django application
In Python scripts, you need to import following library:
from django.utils.translation import ugettext_lazy as _ Then wrapping any text with
_(&amp;#39;TEXT&amp;#39;) will make it translatable.</description>
    </item>
    
    <item>
      <title>Django Rosetta Translations for Django Applications</title>
      <link>https://www.gungorbudak.com/blog/2015/06/12/django-rosetta-translations-for-django/</link>
      <pubDate>Fri, 12 Jun 2015 15:20:33 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/06/12/django-rosetta-translations-for-django/</guid>
      <description>Make a directory called locale/ under the application directory:
cd app_name mkdir locale Add the folder in LOCAL_PATHS dictionary in settings.py:
LOCALE_PATHS = ( os.path.join(PROJECT_ROOT, &amp;#39;app_name&amp;#39;, &amp;#39;locale/&amp;#39;), ) Run the following command to create PO translation file for the application:
python ../manage.py makemessages -l tr -e html,py,txt python ../manage.py compilemessages Option -l is for language, it should match your definition in settings.py:
LANGUAGES = ( (&amp;#39;en&amp;#39; _(&amp;#39;English&amp;#39;)), (&amp;#39;tr&amp;#39; _(&amp;#39;Turkish&amp;#39;)), (&amp;#39;it&amp;#39; _(&amp;#39;Italian&amp;#39;)), ) Repeat the last step for all languages and the go to Rosetta URL to translate.</description>
    </item>
    
    <item>
      <title>Django Rosetta Installation</title>
      <link>https://www.gungorbudak.com/blog/2015/06/12/django-rosetta-installation/</link>
      <pubDate>Fri, 12 Jun 2015 15:07:34 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/06/12/django-rosetta-installation/</guid>
      <description>Install SciPy:
sudo apt-get install python-numpy python-scipy python-matplotlib ipython ipython-notebook python-pandas python-sympy python-nose Install pymongo and nltk:
sudo pip install pymongo sudo pip install nltk Install Python MySQLdb:
sudo apt-get install python-mysqldb Install Rosetta:
sudo pip install django-rosetta Add following into INSTALLED_APPS in settings.py:
&amp;#34;rosetta&amp;#34;, Add following into urls.py:
url(r’^translations/’, include(‘rosetta.urls’)), To also allow language prefixes , change patters to i18n_patterns in urls.py:
urlpatterns += i18n_patterns( ... ) </description>
    </item>
    
    <item>
      <title>Obtaining Molecule Description using Open Babel / PyBel</title>
      <link>https://www.gungorbudak.com/blog/2015/06/07/obtaining-molecule-description-using/</link>
      <pubDate>Sun, 07 Jun 2015 08:51:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/06/07/obtaining-molecule-description-using/</guid>
      <description>Open Babel is a great tool to analyze and investigate molecular data (.MOL, .SDF files). Its Python API is particularly very nice if you are familiar with Python already. In this post, I&amp;rsquo;ll demonstrate how you can obtain molecule description such as molecular weight, HBA, HBD, logP, formula, number of chiral centers using PyBel.
Installation
$ sudo apt-get install openbabel python-openbabel Usage for MW, HBA, HBD, logP
After reading .MOL file, we need to use calcdesc method with descnames argument for getting the descriptions.</description>
    </item>
    
    <item>
      <title>Running Script on Cluster (StarCluster)</title>
      <link>https://www.gungorbudak.com/blog/2015/05/27/running-script-on-cluster-starcluster/</link>
      <pubDate>Wed, 27 May 2015 15:59:32 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/05/27/running-script-on-cluster-starcluster/</guid>
      <description>Start a new cluster with the configuration file you modified:
starcluster start cluster_name Send the script to the running cluster:
starcluster put cluster_name myscr.csh /home/myscr.csh Run it using source:
starcluster sshmaster cluster_name &amp;quot;source /home/myscr.csh &amp;gt;&amp;amp; /home/myscr.log&amp;quot; </description>
    </item>
    
    <item>
      <title>Uploading Files to AWS using SSH/SCP</title>
      <link>https://www.gungorbudak.com/blog/2015/05/09/uploading-files-to-aws-using-sshscp/</link>
      <pubDate>Sat, 09 May 2015 15:18:12 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/05/09/uploading-files-to-aws-using-sshscp/</guid>
      <description>Here is a small command for uploading files to AWS through SSH&amp;rsquo;s command scp (secure copy).
scp -i path/to/your/key-pairs/file path/to/file/you/want/to/upload ubuntu@PUBLIC_DNS:path/to/the/destination </description>
    </item>
    
    <item>
      <title>Errno 13 Permission denied Django File Uploads</title>
      <link>https://www.gungorbudak.com/blog/2015/05/09/errno-13-permission-denied-django-file-uploads/</link>
      <pubDate>Sat, 09 May 2015 15:16:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/05/09/errno-13-permission-denied-django-file-uploads/</guid>
      <description>Run following command to give www-data permissions to static folder and all its content:
cd path/to/your/django/project sudo chown -R www-data:www-data static/ Do this in your production server</description>
    </item>
    
    <item>
      <title>Configuring Mezzanine for Apache server &amp; mod_wsgi in AWS</title>
      <link>https://www.gungorbudak.com/blog/2015/05/09/configuring-mezzanine-for-apache-server-modwsgi/</link>
      <pubDate>Sat, 09 May 2015 13:21:59 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/05/09/configuring-mezzanine-for-apache-server-modwsgi/</guid>
      <description>Install [Mezzanine]({% post_url 2015-05-01-how-to-install-mezzanine-on-ubuntulinux-mint %}), [Apache server]({% post_url 2015-05-08-getting-started-with-your-aws-instance-and %}) and mod_wsgi:
sudo apt-get install libapache2-mod-wsgi sudo a2enmod wsgi Set up a MySQL database for your Mezzanine project
Read [my post on how to set up a MySQL database for a Mezzanine project]({% post_url 2015-05-09-how-to-set-up-a-mysql-database-for-a-mezzanine %})
Collect static files:
python manage.py collectstatic Configure your Apache server configuration for the project like following:
WSGIPythonPath /home/ubuntu/www/mezzanine-project &amp;lt;VirtualHost *:80&amp;gt; #ServerName example.com ServerAdmin admin@example.com DocumentRoot /home/ubuntu/www/mezzanine-project WSGIScriptAlias / /home/ubuntu/www/mezzanine-project/wsgi.</description>
    </item>
    
    <item>
      <title>How to Set Up a MySQL Database for a Mezzanine Project</title>
      <link>https://www.gungorbudak.com/blog/2015/05/09/how-to-set-up-a-mysql-database-for-a-mezzanine/</link>
      <pubDate>Sat, 09 May 2015 13:21:10 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/05/09/how-to-set-up-a-mysql-database-for-a-mezzanine/</guid>
      <description>Install MySQL server and python-mysqldb package:
sudo apt-get install mysql-server sudo apt-get install python-mysqldb Run MySQL:
mysql -u root -p Create a database:
mysql&amp;gt; create database mezzanine_project; Confirm it:
mysql&amp;gt; show databases; Exit:
mysql&amp;gt; exit Configure local_settings.py:
cd path/to/your/mezzanine/projectnano local_settings.py Like following:
DATABASES = { &amp;#34;default&amp;#34;: { &amp;#34;ENGINE&amp;#34;: &amp;#34;django.db.backends.mysql&amp;#34;, &amp;#34;NAME&amp;#34;: &amp;#34;mezzanine_project&amp;#34;, &amp;#34;USER&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;PASSWORD&amp;#34;: &amp;#34;123456&amp;#34;, &amp;#34;HOST&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;PORT&amp;#34;: &amp;#34;&amp;#34;, } } Note: Replace your password</description>
    </item>
    
    <item>
      <title>Setting Up Mezzanine Projects in AWS</title>
      <link>https://www.gungorbudak.com/blog/2015/05/08/setting-up-mezzanine-projects-in-aws/</link>
      <pubDate>Fri, 08 May 2015 13:40:27 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/05/08/setting-up-mezzanine-projects-in-aws/</guid>
      <description>Go to EC2 management console, Security groups and add a Custom TCP inbound rule with port 8000. Select &amp;ldquo;Anywhere&amp;rdquo; from the list.
Then follow [this to install Mezzanine]({% post_url 2015-05-01-how-to-install-mezzanine-on-ubuntulinux-mint %})
Above tutorial is also explains setting up a site record. Mezzanine default site record is 127.0.0.1:8000 which should be 0.0.0.0:8000 in our case. So, enter 0.0.0.0:8000 when you’re asked to enter a site record when you ru
python manage.py createdb Also, you might still need to provide this site record while running the development server:</description>
    </item>
    
    <item>
      <title>Getting Started with Your AWS Instance and Installing and Setting Up an Apache Server</title>
      <link>https://www.gungorbudak.com/blog/2015/05/08/getting-started-with-your-aws-instance-and/</link>
      <pubDate>Fri, 08 May 2015 11:12:06 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/05/08/getting-started-with-your-aws-instance-and/</guid>
      <description>Update and upgrade packages:
sudo apt-get update sudo apt-get upgrade Install Apache server:
sudo apt-get install apache2 Set up a root folder in home folder and create an index file for testing:
mkdir ~/www echo ‘Hello, World!’ &amp;gt; ~/www/index.html Set up your virtual host:
sudo cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/000-www.conf sudo nano /etc/apache2/sites-available/000-www.conf Modify DocumentRoot to point your &amp;ldquo;www&amp;rdquo; folder in home folder (e.g. /home/ubuntu/www)
And add following lines after DocumentRoot line:</description>
    </item>
    
    <item>
      <title>AWS Start an Instance and Connect to it</title>
      <link>https://www.gungorbudak.com/blog/2015/05/08/aws-start-an-instance-and-connect-to-it/</link>
      <pubDate>Fri, 08 May 2015 10:41:02 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/05/08/aws-start-an-instance-and-connect-to-it/</guid>
      <description>Go to EC2 management console
Create a new key-pair if necessary and download it
Launch an instance
Add HTTP security group for web applications over HTTP
Get public DNS
Change permissions on key-pair file:
chmod 400 path/to/your/file.pem Connect:
ssh -i path/to/your/file.pem ubuntu@PUBLIC_DNS Note: ubuntu is for connecting an Ubuntu 64 bit instance. It’s different for others</description>
    </item>
    
    <item>
      <title>How to Get Path to or Directory of Current Script in R</title>
      <link>https://www.gungorbudak.com/blog/2015/05/06/how-to-get-path-to-or-directory-of-current-script/</link>
      <pubDate>Wed, 06 May 2015 22:58:18 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/05/06/how-to-get-path-to-or-directory-of-current-script/</guid>
      <description>Use following code to get the path to or directory of current (running) script in R:
scr_dir &amp;lt;- dirname(sys.frame(1)$ofile) scr_path &amp;lt;- paste(scr_dir, &amp;#34;script.R&amp;#34;, sep=&amp;#34;/&amp;#34;) Taken from SO </description>
    </item>
    
    <item>
      <title>How to Get (or Load) NCBI GEO Microarray Data into R using GEOquery Package from Bioconductor</title>
      <link>https://www.gungorbudak.com/blog/2015/05/06/how-to-get-or-load-ncbi-geo-microarray/</link>
      <pubDate>Wed, 06 May 2015 16:22:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/05/06/how-to-get-or-load-ncbi-geo-microarray/</guid>
      <description>R, especially with lots of Bioconductor packages, provides nice tools to load, manage and analyze microarray data. If you are trying to load NCBI GEO data into R, use GEOquery package. Here, I&amp;rsquo;ll describe how to start with it and probably in my future posts I&amp;rsquo;ll mention more.
Installation
source(&amp;#34;http://bioconductor.org/biocLite.R&amp;#34;) biocLite(&amp;#34;GEOquery&amp;#34;) Usage
library(GEOquery) gds &amp;lt;- getGEO(&amp;#34;GDS5072&amp;#34;) or
library(GEOquery) gds &amp;lt;- getGEO(filename=&amp;#34;path/to/GDS5072.soft.gz&amp;#34;) getGEO function return a complex class type GDS object which contains the complete dataset.</description>
    </item>
    
    <item>
      <title>How to Install Mezzanine on Ubuntu/Linux Mint [Complete Guide]</title>
      <link>https://www.gungorbudak.com/blog/2015/05/01/how-to-install-mezzanine-on-ubuntulinux-mint/</link>
      <pubDate>Fri, 01 May 2015 16:08:33 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/05/01/how-to-install-mezzanine-on-ubuntulinux-mint/</guid>
      <description>Mezzanine is a CMS application built on Django web framework. The installation steps are easy but your environment may not just suitable enough for it work without a problem. So, here I&amp;rsquo;m going to describe complete installation from scratch on a virtual environment.
First of all, install virtualenv:
$ sudo apt-get install python-virtualenv Then, create a virtual environment:
$ virtualenv testenv And, activate it: $ cd testenv $ source bin/activate</description>
    </item>
    
    <item>
      <title>How to Clear (or Drop) DB Table of A Django App</title>
      <link>https://www.gungorbudak.com/blog/2015/04/13/how-to-clear-or-drop-db-table-of-a-django-app/</link>
      <pubDate>Mon, 13 Apr 2015 11:38:41 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/04/13/how-to-clear-or-drop-db-table-of-a-django-app/</guid>
      <description>Let’s say you created a Django app and ran python manage.py syncdb and created its table. Everytime you make a change in the table, you’ll need to drop that table and run python manage.py syncdb again to update. And how you drop a table of a Django app:
$ python manage.py sqlclear app_name | python manage.py dbshell Drop tables of an app with migrations (Django &amp;gt;= 1.8):
$ python manage.py migrate appname zero Recreate all the tables:</description>
    </item>
    
    <item>
      <title>Salmonella - Host Interaction Network - A Detailed, Better Visualization</title>
      <link>https://www.gungorbudak.com/blog/2015/01/21/salmonella-host-interaction-network/</link>
      <pubDate>Wed, 21 Jan 2015 09:35:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/01/21/salmonella-host-interaction-network/</guid>
      <description>We&amp;rsquo;re almost done with the analyses and we&amp;rsquo;re making the final visualization of the network. As I previously posted, the network was clustered and visualized by time points. After that, we have done several more analyses and here I report how we visualized them. I&amp;rsquo;m going to post more about how we did the analyses separately.
First, the nodes are grouped into experimental and not experimental (PCSF nodes). This can easily be done by parsing experimental network output and network outputs of PCSF.</description>
    </item>
    
    <item>
      <title>GO Enrichment of Network Clusters</title>
      <link>https://www.gungorbudak.com/blog/2015/01/16/go-enrichment-of-network-clusters/</link>
      <pubDate>Fri, 16 Jan 2015 20:23:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/01/16/go-enrichment-of-network-clusters/</guid>
      <description>In my previous post, I mentioned how I clustered the network we obtained at the end. For functional annotation gene ontology (GO) enrichment has been done on these clusters.
There were 20 clusters and the HGNC names are obtained separately for each cluster and using DAVID functional annotation tool API, GO and pathway annotations are collected per cluster and these are saved separately.
http://david.abcc.ncifcrf.gov/api.jsp?type=OFFICIAL_GENE_SYMBOL&amp;amp;amp;tool=chartReport&amp;amp;amp;annot=GOTERM_BP_FAT,GOTERM_CC_FAT,GOTERM_MF_FAT,BBID,BIOCARTA,KEGG_PATHWAY&amp;amp;amp;ids=HGNC_NAME1,HGNC_NAME2,HGNC_NAME3,... Above URL was used to obtain chart report for some GO and pathways chart records.</description>
    </item>
    
    <item>
      <title>Network Clustering with NeAT - RNSC Algorithm</title>
      <link>https://www.gungorbudak.com/blog/2015/01/16/network-clustering-with-neat-rnsc/</link>
      <pubDate>Fri, 16 Jan 2015 20:06:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/01/16/network-clustering-with-neat-rnsc/</guid>
      <description>As we have obtained proteins at different times points from the experimental data, then we have found intermediate nodes (from human interactome) using PCSF algorithm and finally with a special matrix from the network that PCSF created, we have validated the edges and also determined edge directions using an approach which a divide and conquer (ILP) approach for construction of large-scale signaling networks from PPI data. The resulting network is a directed network and will be used and visualized for further analyses.</description>
    </item>
    
    <item>
      <title>Finding k-cores and Clustering Coefficient Computation with NetworkX </title>
      <link>https://www.gungorbudak.com/blog/2015/01/16/finding-k-cores-and-clustering/</link>
      <pubDate>Fri, 16 Jan 2015 12:03:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/01/16/finding-k-cores-and-clustering/</guid>
      <description>Assume you have a large network and you want to find k-cores of each node and also you want to compute clustering coefficient for each one. Python package NetworkX comes with very nice methods for you to easily do these.
k-core is a maximal subgraph whose nodes are at least k degree [1]. To find k-cores:
Add all edges you have in your network in a NetworkX graph, and use core_number method that gets graph as the single input and returns node – k-core pairs.</description>
    </item>
    
    <item>
      <title>Searching Open Reading Frames (ORF) in DNA sequences - ORF Finder</title>
      <link>https://www.gungorbudak.com/blog/2015/01/14/searching-open-reading-frames-orf-in/</link>
      <pubDate>Wed, 14 Jan 2015 19:46:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/01/14/searching-open-reading-frames-orf-in/</guid>
      <description>Open reading frames (ORF) are regions on DNA which are translated into protein. They are in between start and stop codons and they are usually long.
The Python script below searches for ORFs in six frames and returns the longest one. It doesn&amp;rsquo;t consider start codon as a delimiter and only splits the sequence by stop codons. So the ORF can start with any codon but ends with a stop codon (TAG, TGA, TAA).</description>
    </item>
    
    <item>
      <title>Reconstructed Salmonella Signaling Network Visualized and Colored</title>
      <link>https://www.gungorbudak.com/blog/2015/01/14/reconstructed-salmonella-signaling/</link>
      <pubDate>Wed, 14 Jan 2015 19:12:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/01/14/reconstructed-salmonella-signaling/</guid>
      <description>After fold changes were obtained and HGNC names were found for each phosphopeptide, these were used to construct Salmonella signaling network using PCSF and then with the nodes that PCSF found as well, we generated a matrix which has node in the rows and time points in the columns and each cell shows the presence of corresponding protein under the corresponding time point(s).
The matrix has 658 nodes (proteins) and 4 time points as indicated before: 2 min, 5 min, 10 min and 20 min.</description>
    </item>
    
    <item>
      <title>Python: Get Longest String in a List</title>
      <link>https://www.gungorbudak.com/blog/2015/01/13/python-get-longest-string-in-list/</link>
      <pubDate>Tue, 13 Jan 2015 08:17:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/01/13/python-get-longest-string-in-list/</guid>
      <description>Here is a quick Python trick you might use in your code.
Assume you have a list of strings and you want to get the longest one in the most efficient way.
&amp;gt;&amp;gt;&amp;gt;l=[&amp;#34;aaa&amp;#34;, &amp;#34;bb&amp;#34;, &amp;#34;c&amp;#34;] &amp;gt;&amp;gt;&amp;gt;longest_string = max(l, key = len) &amp;gt;&amp;gt;&amp;gt;longest_string &amp;#39;aaa&amp;#39; </description>
    </item>
    
    <item>
      <title>Python: defaultdict(list) Dictionary of Lists</title>
      <link>https://www.gungorbudak.com/blog/2015/01/12/python-defaultdictlist-dictionary-of/</link>
      <pubDate>Mon, 12 Jan 2015 09:29:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/01/12/python-defaultdictlist-dictionary-of/</guid>
      <description>Most of the time, when you need to work on large data, you&amp;rsquo;ll have to use some dictionaries in Python. Dictionaries of lists are very useful to store large data in very organized way. You can always initiate them by initiating empty lists inside an empty dictionary but when you don&amp;rsquo;t know how many of them you&amp;rsquo;ll end up with and if you want an easier option, use defaultdict(list). You just need to import it, first:</description>
    </item>
    
    <item>
      <title>Python: extend() Append Elements of a List to a List</title>
      <link>https://www.gungorbudak.com/blog/2015/01/12/python-extend-append-elements-of-list/</link>
      <pubDate>Mon, 12 Jan 2015 08:58:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/01/12/python-extend-append-elements-of-list/</guid>
      <description>When you append a list to a list by using append() method, you&amp;rsquo;ll see your list is going to be appended as a list:
&amp;gt;&amp;gt;&amp;gt;l=[&amp;#34;a&amp;#34;] &amp;gt;&amp;gt;&amp;gt;l2=[&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;] &amp;gt;&amp;gt;&amp;gt;l.append(l2) &amp;gt;&amp;gt;&amp;gt;l [&amp;#39;a&amp;#39;, [&amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;]] If you want to append elements of the list directly without creating nested lists, use extend() method:
&amp;gt;&amp;gt;&amp;gt;l=[&amp;#34;a&amp;#34;] &amp;gt;&amp;gt;&amp;gt;l2=[&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;] &amp;gt;&amp;gt;&amp;gt;l.extend(l2) &amp;gt;&amp;gt;&amp;gt;l [&amp;#39;a&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;] </description>
    </item>
    
    <item>
      <title>Salmonella Data Preprocessing for PCSF Algorithm</title>
      <link>https://www.gungorbudak.com/blog/2015/01/12/salmonella-data-preprocessing-for-pcsf/</link>
      <pubDate>Mon, 12 Jan 2015 08:47:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/01/12/salmonella-data-preprocessing-for-pcsf/</guid>
      <description>This post describes data preprocessing in Salmonella project for Prize-Collecting Steiner Forest Problem (PCSF) algorithm.
Salmonella data taken from Table S6 in Phosphoproteomic Analysis of Salmonella-Infected Cells Identifies Key Kinase Regulators and SopB-Dependent Host Phosphorylation Events by Rogers, LD et al. has been converted to tab delimited TXT file from its original XLS file for easy reading in Python.
The data should be separated into time points files (2, 5, 10 and 20 minutes) each of which will contain corresponding phophoproteins and their fold changes.</description>
    </item>
    
    <item>
      <title>UPGMA Algorithm Described - Unweighted Pair-Group Method with Arithmetic Mean</title>
      <link>https://www.gungorbudak.com/blog/2015/01/10/upgma-algorithm-described-unweighted/</link>
      <pubDate>Sat, 10 Jan 2015 08:41:00 +0300</pubDate>
      
      <guid>https://www.gungorbudak.com/blog/2015/01/10/upgma-algorithm-described-unweighted/</guid>
      <description>UPGMA is an agglomerative clustering algorithm that is ultrametric (assumes a molecular clock - all lineages are evolving at a constant rate) by Sokal and Michener in 1958.
The idea is to continue iteration until only one cluster is obtained and at each iteration, join two nearest clusters (which become a higher cluster). The distance between any two clusters are calculated by averaging distances between elements of each cluster.
To understand better, see UPGMA worked example by Dr Richard Edwards.</description>
    </item>
    
  </channel>
</rss>
